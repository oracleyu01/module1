"""
LangGraph ì›Œí¬í”Œë¡œìš°
"""
from typing import TypedDict, Annotated, List, Union, Dict
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
import operator
import streamlit as st
import time
from supabase import create_client
from config.settings import get_api_keys
from core.crawler import ProConsLaptopCrawler

# State ì •ì˜
class SearchState(TypedDict):
    """ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ"""
    product_name: str
    search_method: str
    results: dict
    pros: List[str]
    cons: List[str]
    sources: List[dict]
    messages: Annotated[List[Union[HumanMessage, AIMessage]], operator.add]
    error: str

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_supabase_client():
    keys = get_api_keys()
    return create_client(keys["SUPABASE_URL"], keys["SUPABASE_KEY"]) if keys["SUPABASE_URL"] and keys["SUPABASE_KEY"] else None

@st.cache_resource
def get_crawler():
    keys = get_api_keys()
    return ProConsLaptopCrawler(keys["NAVER_CLIENT_ID"], keys["NAVER_CLIENT_SECRET"]) if keys["NAVER_CLIENT_ID"] and keys["NAVER_CLIENT_SECRET"] else None

def search_database(state: SearchState) -> SearchState:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œí’ˆ ê²€ìƒ‰"""
    product_name = state["product_name"]
    supabase = get_supabase_client()
    
    if not supabase:
        state["messages"].append(
            AIMessage(content="âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        )
        state["results"] = {"data": None}
        return state
    
    state["messages"].append(
        HumanMessage(content=f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ê²€ìƒ‰ ì¤‘...")
    )
    
    try:
        exact_match = supabase.table('laptop_pros_cons').select("*").eq('product_name', product_name).execute()
        if exact_match.data:
            state["search_method"] = "database"
            state["results"] = {"data": exact_match.data}
            state["messages"].append(
                AIMessage(content=f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! ({len(exact_match.data)}ê°œ í•­ëª©)")
            )
            return state
        
        state["messages"].append(
            AIMessage(content=f"âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        )
        state["results"] = {"data": None}
        return state
        
    except Exception as e:
        state["error"] = str(e)
        state["messages"].append(
            AIMessage(content=f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        )
        state["results"] = {"data": None}
        return state

def crawl_web(state: SearchState) -> SearchState:
    """ì›¹ì—ì„œ ì œí’ˆ ì •ë³´ í¬ë¡¤ë§"""
    if state["results"].get("data"):
        return state
    
    product_name = state["product_name"]
    state["search_method"] = "web_crawling"
    crawler = get_crawler()
    
    if not crawler:
        state["messages"].append(
            AIMessage(content="âš ï¸ ì›¹ í¬ë¡¤ë§ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        )
        return state
    
    state["messages"].append(
        HumanMessage(content=f"ğŸŒ ì›¹ì—ì„œ '{product_name}' ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...")
    )
    
    # API í‚¤ê°€ ì—†ì„ ë•Œ ìƒ˜í”Œ ë°ì´í„°
    keys = get_api_keys()
    if not keys["OPENAI_API_KEY"]:
        state["pros"] = [
            "ê°€ë³ê³  íœ´ëŒ€ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤",
            "ë°°í„°ë¦¬ ì§€ì† ì‹œê°„ì´ ê¹ë‹ˆë‹¤",
            "ë””ìŠ¤í”Œë ˆì´ê°€ ì„ ëª…í•©ë‹ˆë‹¤",
            "ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤",
            "ë””ìì¸ì´ ì„¸ë ¨ë˜ì—ˆìŠµë‹ˆë‹¤"
        ]
        state["cons"] = [
            "ê°€ê²©ì´ ë¹„ìŒ‰ë‹ˆë‹¤",
            "í¬íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
            "í‚¤ë³´ë“œ í‚¤ê°ì´ ì•„ì‰½ìŠµë‹ˆë‹¤"
        ]
        state["messages"].append(
            AIMessage(content="ğŸ“Œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤ (API í‚¤ ì„¤ì • í•„ìš”)")
        )
        return state
    
    all_pros = []
    all_cons = []
    sources = []
    
    search_queries = [
        f"{product_name} ì¥ë‹¨ì  ì‹¤ì‚¬ìš©",
        f"{product_name} ë‹¨ì  í›„ê¸°",
        f"{product_name} ì¥ì  ë¦¬ë·°"
    ]
    
    for query in search_queries:
        state["messages"].append(
            AIMessage(content=f"ğŸ” ê²€ìƒ‰ì–´: '{query}'")
        )
        
        result = crawler.search_blog(query, display=10)
        if not result or 'items' not in result:
            continue
        
        posts = result['items']
        state["messages"].append(
            AIMessage(content=f"â†’ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ë°œê²¬")
        )
        
        for idx, post in enumerate(posts[:5]):
            state["messages"].append(
                AIMessage(content=f"ğŸ“– ë¶„ì„ ì¤‘: {post['title'][:40]}...")
            )
            
            content = crawler.crawl_content(post['link'])
            if not content:
                continue
            
            crawler.stats['total_crawled'] += 1
            
            pros_cons = crawler.extract_pros_cons_with_gpt(product_name, content)
            
            if pros_cons:
                all_pros.extend(pros_cons['pros'])
                all_cons.extend(pros_cons['cons'])
                sources.append({
                    'title': post['title'],
                    'link': post['link'],
                    'date': post.get('postdate', '')
                })
                
                state["messages"].append(
                    AIMessage(content=f"âœ“ ì¥ì  {len(pros_cons['pros'])}ê°œ, ë‹¨ì  {len(pros_cons['cons'])}ê°œ ì¶”ì¶œ")
                )
            
            time.sleep(1)
        
        time.sleep(2)
    
    unique_pros = crawler.deduplicate_points(all_pros)
    unique_cons = crawler.deduplicate_points(all_cons)
    
    state["pros"] = unique_pros
    state["cons"] = unique_cons
    state["sources"] = sources[:10]
    
    if state["pros"] or state["cons"]:
        state["messages"].append(
            AIMessage(content=f"ğŸ‰ ì›¹ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ ìˆ˜ì§‘")
        )
        
        # DBì— ì €ì¥
        try:
            supabase = get_supabase_client()
            if supabase:
                data = []
                
                for pro in state["pros"]:
                    data.append({
                        'product_name': product_name,
                        'type': 'pro',
                        'content': pro
                    })
                
                for con in state["cons"]:
                    data.append({
                        'product_name': product_name,
                        'type': 'con',
                        'content': con
                    })
                
                if data:
                    supabase.table('laptop_pros_cons').insert(data).execute()
                    state["messages"].append(
                        AIMessage(content="ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì™„ë£Œ!")
                    )
                    st.session_state.saved_products += 1
        except Exception as e:
            state["messages"].append(
                AIMessage(content=f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            )
    else:
        state["messages"].append(
            AIMessage(content=f"ğŸ˜¢ '{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        )
    
    state["messages"].append(
        AIMessage(content=f"ğŸ“Š í¬ë¡¤ë§ í†µê³„: ì´ {crawler.stats['total_crawled']}ê°œ í˜ì´ì§€, ìœ íš¨ ì¶”ì¶œ {crawler.stats['valid_pros_cons']}ê°œ")
    )
    
    return state

def process_results(state: SearchState) -> SearchState:
    """ê²°ê³¼ ì²˜ë¦¬ ë° ì •ë¦¬"""
    if state["search_method"] == "database" and state["results"].get("data"):
        data = state["results"]["data"]
        state["pros"] = [item['content'] for item in data if item['type'] == 'pro']
        state["cons"] = [item['content'] for item in data if item['type'] == 'con']
        state["sources"] = []
        
        state["messages"].append(
            AIMessage(content=f"ğŸ“‹ ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ: ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ")
        )
    
    return state

def should_search_web(state: SearchState) -> str:
    """ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨"""
    if state["results"].get("data"):
        return "process"
    else:
        return "crawl"

@st.cache_resource
def create_search_workflow():
    """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_db", search_database)
    workflow.add_node("crawl_web", crawl_web)
    workflow.add_node("process", process_results)
    
    # ì—£ì§€ ì„¤ì •
    workflow.set_entry_point("search_db")
    workflow.add_conditional_edges(
        "search_db",
        should_search_web,
        {
            "crawl": "crawl_web",
            "process": "process"
        }
    )
    workflow.add_edge("crawl_web", "process")
    workflow.add_edge("process", END)
    
    return workflow.compile()
